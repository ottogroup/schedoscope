#
# Schedoscope configuration properties and default values
#

schedoscope {

    #
    # Basic settings
    #

    app {
    
        #
        # The environment name of your Schedoscope instance. This allows you to run 
        # multiple instances of schedoscope, such as for development, testing or 
        # production but also for different applications / datawarehouses.
        # 
        # The environment impacts the default database and partition path naming 
        # schemes.
        # 
        # Common environment names are: dev, int, prod
        #
    
        environment = "dev"
        
        #
        # View augmentor (a class extending 
        # org.schedoscope.dsl.views.ViewUrlParser.ParsedViewAugmentor) 
        # to use for dynamic view instantiation via the view pattern URL scheme. 
        # This can be used to modify instantiated views in application-specific 
        # ways outside of view pattern URLs. Should rarely need change.
        #
        
        parsedViewAugmentorClass = "org.schedoscope.dsl.views.NoAugmentation"
    }

    #
    # Settings with regard to scheduling logic
    #

    scheduler {
    
        #
        # The earliest date to consider for MonthlyParameterization or 
        # DailyParameterization view traits. Should be the earliest day you have 
        # data for your application to avoid any needless no-data overhead.
        #
    
        earliestDay = "2013-12-01"
        
        #
        # The latest date to consider for MonthlyParameterization or 
        # DailyParameterization view traits. Should usually be "now", but can be 
        # reduced to a fixed date, for example in testing environments.
        #
        
        latestDay   = "now"
    
    
        #
        # Various timeouts to obey
        #
        
        timeouts {
            
            #
            # Timeout waiting for metastore operations
            #
            
            schema = 1 hour
           
            
            #
            # Timeout waiting for scheduling command to return
            #
            
            schedulingCommand = 1 hour
        }
    }

    #
    # Configuration properties with regard Schedoscope's REST web service API.
    # Used for both Schedoscope server and the Schedoscope client tool.
    #

    webservice {
    
        
        #
        # Host where Schedoscope instance is running
        #
        
        host = "localhost"
        
        #
        # Port on which the Schedoscope web service listens for connections
        #
        
        port = 20698
        
        #
        # Directory for storing static web service resources
        #
        
        resourceDirectory = "www"
        
        #
        # Number of webservice actors to answer requests.
        #
        
        concurrency = 5
    }


    #
    # Kerberos-related settings
    #

    kerberos {
    
        #
        # The kerberos principal required for access to the Metastore
        # E.g.: hive/somemachine@SOMEDOMAIN.COM
        #
        principal = ""
    }

    #
    # Settings related to core Hadoop
    #
    
    hadoop {
    
        #
        # Address and port of the YARN resource manager. Note that any locally configured Hadoop environment
        # takes precedence.
        #
        
        resourceManager = "localhost:8032"
        
        #
        # Address and port of the HDFS namenode. Note that any locally configured Hadoop environment
        # takes precedence.
        #
        
        nameNode = "localhost:8020"     
        
        #
        # HDFS URI
        #
        
        hdfs = "hdfs://localhost:8020"   
    }


    #
    # Settings related to the Hive metastore
    #
    
    metastore {
    
        #
        # Thrift URI to metastore service
        #
        metastoreUri = "thrift://localhost:9083"
        
        #
        # Hive JDBC URL to connect to Hive Server 2
        #
        
        jdbcUrl = "jdbc:hive2://localhost:10000/default"
        
        #
        # Number of parallel connections to use for schema- and partition creation
        # via the Hive metastore
        #
        
        concurrency = 5
        
        #
        # Number of partitions to put into a batch for each connection when 
        # creating partitions
        #
        
        writeBatchSize = 2500 
        
        #
        # Number of partitions to read in batch with each request on each 
        # connection
        #
        
        readBatchSize = 10000
    }
    
    #
    # Settings concerning automatic logic change detection
    #
    
    versioning {
    
        #
        # Compute transformation version checksums and detect need for 
        # rematerialization based on those checksums. Should be kept as is.
        #
        
        transformations = true  
    }
    
    #
    # Number of retries for transformations before view state is changed to 
    # failed.
    #
    
    action.retry = 5

    #
    # Default settings for exportTo() exporters.
    #
    
    export {
        
        #
        # Export settings for all export types
        #

        salt = "vD75MqvaasIlCf7H"

        #
        # JDBC exporter settings.
        #
        
        jdbc {
        
            #
            # Number of reducers to use for parallel writing to JDBC.
            #   
            
            numberOfReducers = 10
        
            #
            # Batch size for insert statements to use
            #
            
            insertBatchSize = 10000
            
            #
            # Storage engine to use (in case of MySQL JDBC targets)
            #
            
            storageEngine = "InnoDB"
        
        }
        
        #
        # Redis exporter.
        #
        
        redis {
        
            #
            # Number of reducers to use for parallel writing to Redis.
            #   
            
            numberOfReducers = 10
            
            #
            # Usage of pipeline mode for writing to Redis.
            #
            
            usePipelineMode = false
            
            #
            # Batch size for Redis pipeline mode
            #
            
            insertBatchSize = 10000
        
        }

        #
        # Kafka exporter
        #

        kafka {

            #
            # Number of reducer to use for parallel writing to Kafka.
            #

            numberOfReducers = 10

        }
    }
    
    #
    # Metascope related settings
    #
    
    metascope {
  
      #
      # Metascope webservice port
      #
  
      port = 8080
    
      #
      # Configure authentication method
      #
  
      auth {
  
        #
        # Possible methods = ["simple", "ldap"]
        # simple: custom user management
        # ldap: external ldap directory is used for authentication
        #
  	
        authentication = "simple"
  	
        #
        # ldap settings
        #
  	
        ldap {
  	  
          #
          # ldap server url
          #
  	  
          url = ""
  	  
          #
          #	ldap manager dn
          #
  	  
          managerDn = ""
  	  
          #
          # ldap manager password
          #
  	  
          managerPassword = ""
  	  
          #
          # ldap user dn pattern
          #	
  	  
          userDnPattern = ""
  	  
          #
          # ldap group search base
          #
  	  
          groupSearchBase = ""
  	  
          #
          # Allowed groups to access Metascope
          #
  	  
          allowedGroups = ""
  	  
          #
          #	Admin group(s): User will have admin rights
          #
  	  
          adminGroups = ""
      
        }
  	
      }

      #
      # Database which is used as the metadata repository.
      # Default: embedded Apache Derby instance, which creates a local repository in the metascope directory ('.../metascope/repository')
      #

      repository {

        #
        # JDBC URL to database server
        #

        url = "jdbc:derby:{metascope.dir}/repository;create=true"

        #
        # database user
        #

        user = "sa"

        #
        # users password
        #

        password = ""

        #
        # Dialect to use to talk to database. For further information, visit:
        # http://docs.jboss.org/hibernate/orm/3.5/javadocs/org/hibernate/dialect/package-summary.html
        #

        dialect = "org.hibernate.dialect.DerbyTenSevenDialect"
      }

      #
      # Settigns relataed to the Apache Solr instance
      #

      solr {

        #
        # URL to Solr instance. 
        # Default: embedded Apache Solr instance, which creates a local index in the metascope directory ('.../metascope/solr')
        #

        url="{metascope.dir}/solr"
      }

      #
      # General settings concerning logging (via SLF4J) 
      #

      logging {

        #
        # Path to logfile
        #

        logfile = "{metascope.dir}/logs/metascope-service.log"

        #
        # Log level granularity
        #

        loglevel = "DEBUG"
      }

    }
    
    #
    # Driver settings for the different transformation types. 
    #
    
    transformations = {
    
        #
        # Settings for Hive transformations
        #
        
        hive : {
            
            #
            # Location where to put Hive UDFS in HDFS
            #
            
            location = "/tmp/schedoscope/hive/"
            
            #
            # Comma-separated list of directories where additional UDF library 
            # jars can be found that are to be put onto HDFS when launching 
            # Schedoscope.
            #
            
            libDirectory = ""
            
            #
            # Hive JDBC URL to use for issueing queries. Should equal 
            # schedoscope.metastore.jdbcUrl
            #
            
            url = "jdbc:hive2://localhost:10000/default"
            
            #
            # Do not change. Hive UDF jars should not be unpacked in HDFS.
            #
            
            unpack = false
            
            #
            # Number of parallel Driver actors to use for executing Hive 
            # transformations
            #
            
            concurrency = 10
            
            #
            # Timeout for Hive transformations.
            #
            
            timeout = 1 day
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
            
        },
        
        #
        # Settings for Pig transformations
        #
        
        pig : {
        
            #
            # Location where to put Pig library jar in HDFS
            #
        
            location = "/tmp/soda/pig/"
            
            #
            # Ignored
            #
            
            libDirectory = ""
            
            #
            # Ignored.
            #
            
            url = ""
            
            #
            # Do not change. Pig jars should not be unpacked in HDFS.
            #
            
            unpack = false
            
            #
            # Number of parallel Driver actors to use for executing Pig 
            # transformations
            #
            
            concurrency = 10
            
            #
            # Timeout for Pig transformations.
            #
            
            timeout = 1 day
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
            
        }, 
        
        #
        # Settings for MapReduce transformations
        #
        
        mapreduce : {
        
            #
            # Ignored.
            #
            
            location = "/tmp/schedoscope/mapreduce/"
            
            
            #
            # Ignored.
            #
            
            libDirectory = ""
            
            #
            # Ignored.
            #
            
            url = ""
            
            #
            # Ignored
            #
            
            unpack = false
            
            #
            # Number of parallel Driver actors to use for executing MapReduce 
            # transformations
            #
                              
            concurrency = 10
            
            #
            # Timeout for MapReduce transformations
            #
            
            timeout = 1 day
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
                    
        },       
                  
        #
        # Oozie Driver settings
        #
                  
        oozie : {
        
            #
            # Where to put Oozie bundles in HDFS
            #
            
            location = "/tmp/schedoscope/oozie/"
            
            #
            # Comma-separated list of directories where additional Oozie workflow 
            # bundle jars can be found that are to be put onto HDFS when launching 
            # Schedoscope.
            #
            
            libDirectory = ""
            
            #
            # URL of Oozie Server
            #
            
            url = "http://localhost:11000/oozie"
            
            #
            # Number of parallel Driver actors to use for executing Oozie 
            # transformations
            #
            
            concurrency = 10
            
            #
            # Oozie bundle jars need to be unpacked in HDFS.
            #
            
            unpack = true
            
            #
            # Timeout for Oozie transformations
            #
            
            timeout = 1 day
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
            
        },   
        
        #
        # File system driver settings
        #
         
        filesystem : {
        
            #
            # Number of parallel Driver actors to use for executing file system 
            # transformations
            #
        
            concurrency = 1 
            
            #
            # Ignored.
            #
            
            location = "/"
            
            #
            # Ignored
            #
            
            libDirectory = ""

            #
            # Ignored
            #
            
            url = ""
            
            #
            # Ignored
            #
            
            unpack = false
            
            #
            # Timeout for file system transformations
            #
            
            timeout = 1 hour
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
            
        },
        
        #
        # Sequence driver settings
        #
         
        seq : {
        
            #
            # Number of parallel Driver actors to use for executing seq
            # transformations
            #
        
            concurrency = 10
            
            #
            # Ignored.
            #
            
            location = "/"
            
            #
            # Ignored
            #
            
            libDirectory = ""

            #
            # Ignored
            #
            
            url = ""
            
            #
            # Ignored
            #
            
            unpack = false
            
            #
            # Timeout for seq transformations
            #
            
            timeout = 1 day
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
            
        },
        
        #
        # Shell driver settings
        #
            
        shell : {
        
            #
            # Number of parallel Shell Driver actors to use
            #
            
            concurrency = 1
            
            #
            # Ignored
            #
            
            location = "/"
            
            #
            # Ignored
            #
            
            libDirectory = ""
            
            #
            # Ignored
            #
            
            url = ""
            
            #
            # Ignored
            #
            
            unpack = false
            
            #
            # Timeout for Shell transformations
            #
            
            timeout = 1 day
            
            #
            # The handlers being notified after each driver run has 
            # finished (succeeded or failed). These must implement the
            # trait org.schedoscope.scheduler.driver.DriverRunCompletionHandler
            #
            
            driverRunCompletionHandlers = ["org.schedoscope.scheduler.driver.DoNothingCompletionHandler"]
        } 
          
    }
}

#
# This section covers general Akka settings, in particular concerning the 
# various dispatchers / threadpools.
#

akka {

    #
    # General settings concerning logging (via SLF4J). Logback is given as a 
    # default dependency to Schedoscope so SLF4J via Logback is the default 
    # logging system.
    #

    log-config-on-start = off
    log-dead-letters = off
    loglevel = "INFO"
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    
    #
    # Dispatcher settings
    #
    
    actor {        
    
        guardian-supervisor-strategy = "org.schedoscope.TerminatingStoppingStrategy"
    
        #
        # Given the variety of component-specific dispatchers used by Schedoscope
        # we limit the Akka default dispatcher to 4 threads.
        #
        
        default-dispatcher {
            executor = "thread-pool-executor"
            type = Dispatcher
                                 
            thread-pool-executor {
                core-pool-size-min = 4
                core-pool-size-factor = 1.0
                core-pool-size-max = 4
                task-queue-size = -1
            }

            throughput = 5
        }


        #
        # Configuration of threads to use for unspecific future calls and 
        # computations.
        # Again limited.
        #

        future-call-dispatcher {
            type = Dispatcher
            executor = "thread-pool-executor"                
             
            thread-pool-executor {
                core-pool-size-min = 4
                core-pool-size-factor = 1.0
                core-pool-size-max = 4
                task-queue-size = -1
            }

            throughput = 5
        }


        #
        # The supervisor / message router for the Metastore-related actors is 
        # assigned to one pinned dispatcher / thread. Should not need to be 
        # changed.
        #

        schema-manager-dispatcher {
            executor = "thread-pool-executor"
            type = PinnedDispatcher         
        }           

        #
        # Threadpool / dispatcher configuration for the actor communicating 
        # with the Metastore for creating / reading tables and partitions. Should 
        # be aligned with the property schedoscope.metastore.concurrency 
        #
        
        partition-creator-dispatcher {
            executor = "thread-pool-executor"
            type = Dispatcher
                             
            thread-pool-executor {
                core-pool-size-min = 5
                core-pool-size-factor = 1.0
                core-pool-size-max = 5
                task-queue-size = -1
            }

            throughput = 5
        }


        #
        # The actor logging transformation metadata (version, timestamp) to the 
        # Metastore has one dedicated pinned dispatcher / thread.
        #

        metadata-logger-dispatcher {
            executor = "thread-pool-executor"
            type = PinnedDispatcher         
        }
        
        #
        # The actor managing, assigning, and collecting status information about 
        # transformations to execute has one dedicated pinned dispatcher / thread 
        # for reasons of responsiveness.
        #

        transformation-manager-dispatcher {
            executor = "thread-pool-executor"
            type = PinnedDispatcher         
        }

        #
        # The actor managing and instantiating view actors for scheduling state 
        # management has one dedicated pinned dispatcher / thread for reasons of 
        # responsiveness.
        #

        view-manager-dispatcher {
            executor = "thread-pool-executor"
            type = PinnedDispatcher         
        }        

        #
        # The threadpool / dispatcher used by the view actors for communicating 
        # with each other for view materialization and scheduling.
        #

        views-dispatcher {
            executor = "fork-join-executor"
            type = Dispatcher
                                 
            fork-join-executor {
                parallelism-min = 8 
                parallelism-factor = 2.0
                parallelism-max = 8
                task-queue-size = -1
            }

            throughput = 5
        }          

        #
        # The threadpool / dispatcher available for the transformation drivers. 
        # As drivers execute transformations asynchronously - either by 
        # communicating with servers like Oozie or the Resource Manager or by 
        # futures, this pool should not need to grow much when increasing 
        # concurrency settings of the various transformations.
        #

        driver-dispatcher {
            executor = "thread-pool-executor"
            type = Dispatcher
                             
            thread-pool-executor {
                core-pool-size-min = 8
                core-pool-size-factor = 2.0
                core-pool-size-max = 8
                task-queue-size = -1
            }

            throughput = 5
        }           

        #
        # Due to API limitations, we can currently only execute file system, Pig, 
        # and Hive transformations asynchronously by employing futures. 
        #
        # The following threadpool / dispatcher feeds those futures. Hence, it 
        # should grow proportionally with the concurrency settings of the 
        # mentioned transformation types.
        #

        future-driver-dispatcher {
            type = Dispatcher
            executor = "thread-pool-executor"  

            thread-pool-executor {
                core-pool-size-min = 16
                core-pool-size-factor = 4.0
                core-pool-size-max = 16
                task-queue-size = -1
            }

            throughput = 5
        }
    }
}

#
# The following are a list of "suggestions" for spray settings to use for the 
# Schedoscope web service and client. Akka will not pick them up automatically. 
# They need to be copied and potentially adapted in your schedoscope.conf / 
# application conf. As Schedoscope web service calls can take a bit to return a 
# result and results can be quite large, the settings naively disable timeouts and 
# payload limits.
#

spray.can.client.parsing.max-content-length=100000000
spray.can.client.request-timeout = infinite
spray.can.client.connecting-timeout = infinite
spray.can.client.idle-timeout = infinite
spray.can.host-connector.idle-timeout = infinite
spray.can.server.request-timeout = infinite
spray.can.server.idle-timeout = infinite
